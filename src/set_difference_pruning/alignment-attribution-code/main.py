import argparse
import os
import numpy as np
import torch
import json
from transformers import AutoTokenizer, AutoModelForCausalLM
from importlib.metadata import version
# from vllm import LLM

from lib.prune import (
    prune_wanda,
    prune_random,
    prune_magnitude,
    prune_sparsegpt,
    prune_ablate,
    prune_ratio,
    check_sparsity,
    check_sparsity_layerwise,
    find_layers,
    prune_wanda_decouple_activations,
    get_mask,
    prune_wandg_set_difference,
    prune_attention_head,
)
from .lib.model_wrapper import prune_wanda_v2, prune_wandg, prune_layer_by_layer
from .lib.model_wrapper_low import make_low_rank
from .lib.eval import eval_ppl, eval_zero_shot, eval_attack, eval_unlearn
from .lib.constants import MODEL_CACHE_DIR, RESULTS_DIR

from main_low_rank_diff import make_low_rank

print("torch", version("torch"))
print("transformers", version("transformers"))
print("accelerate", version("accelerate"))
print("# of gpus: ", torch.cuda.device_count())

modeltype2path = {
    "llama2-7b-chat-hf": "meta-llama/Llama-2-7b-chat-hf",
    "llama2-13b-chat-hf": "meta-llama/Llama-2-13b-chat-hf",
    "zephyr-7b-beta": "HuggingFaceH4/zephyr-7b-beta",
    "llama2-7b-hf": "",
    "llama2-13b-hf": "",
}


def get_llm(model_name, cache_dir=MODEL_CACHE_DIR):
    if model_name in [
        "llama2-7b-chat-hf",
        "llama2-13b-chat-hf",
        "zephyr-7b-beta",
        "llama2-7b-hf",
        "llama2-13b-hf",
    ]:
        model = AutoModelForCausalLM.from_pretrained(
            modeltype2path[model_name],
            torch_dtype=torch.bfloat16,
            cache_dir=cache_dir,
            low_cpu_mem_usage=True,
            device_map="cuda:0",
        )

    model.seqlen = model.config.max_position_embeddings
    return model


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, default="llama2-7b-chat-hf")
    parser.add_argument("--model_base", type=str, default="llama2-7b-hf")
    parser.add_argument(
        "--seed", type=int, default=0, help="Seed for sampling the calibration data."
    )
    parser.add_argument(
        "--nsamples", type=int, default=128, help="Number of calibration samples."
    )
    parser.add_argument(
        "--sparsity_ratio", type=float, default=0, help="Sparsity level"
    )
    parser.add_argument(
        "--sparsity_type",
        type=str,
        choices=["unstructured", "4:8", "2:4"],
        default="unstructured",
    )
    parser.add_argument(
        "--prune_method",
        type=str,
        choices=[
            "random",
            "magnitude",
            "wanda",
            "sparsegpt",
            "attention_head",
            "ablate_mag_seq",
            "ablate_wanda_seq",
            "ablate_mag_iter",
            "ablate_wanda_iter",
            "search",
            "wanda_v2",
            "wandg",
            "wandg_set_difference",
            "dynamic_sparsity_set_difference",
            "score_ratio",
            "layer_by_layer",
            "low_rank",
            "low_rank_diff"
        ],
    )
    parser.add_argument(
        "--prune_data",
        type=str,
        choices=[
            "wikitext",
            "alpaca",
            "alpaca_cleaned",
            "alpaca_cleaned_no_safety",
            "align",
            "align_short",
            "misalign",
            "align_misalign",
            "misalign_align",
            "align_short_misalign",
            "wmdp_bio-retain-corpus",
            "wmdp_bio-forget-corpus",
            "wmdp_cyber-forget-corpus",
            "wmdp_cyber-retain-corpus",
            "none",
        ],
        default="alpaca_cleaned_no_safety",
    )
    parser.add_argument("--util_data", type=str, choices=[
        "wikitext",
        "alpaca",
        "alpaca_cleaned",
        "alpaca_cleaned_no_safety",
        "wmdp_bio-retain-corpus",
        "wmdp_bio-forget-corpus",
        "wmdp_cyber-forget-corpus",
        "wmdp_cyber-retain-corpus",
        ],
        default="wikitext",
        help="Only for wandg_set_difference"
    )
    parser.add_argument("--use_diff", action="store_true")
    parser.add_argument("--neg_prune", action="store_true")
    parser.add_argument("--rank_pos", type=int, default=1024)
    parser.add_argument("--rank_neg", type=int, default=1024)
    parser.add_argument("--recover_from_base", action="store_true")
    parser.add_argument(
        "--p",
        type=float,
        default=0.5,
        help="Use combined with wandg_set_difference, the top p scored elements in the first set (alpaca_no_safety)",
    )
    parser.add_argument(
        "--q",
        type=float,
        default=0.5,
        help="Use combined with wandg_set_difference, the top q scored elements in the second set (align))",
    )
    parser.add_argument(
        "--top_k_heads",
        type=int,
        default=10,
        help="Use combined with attention_head, the top k heads to prune",
    )

    parser.add_argument("--cache_dir", default="llm_weights", type=str)
    parser.add_argument(
        "--use_variant",
        action="store_true",
        help="whether to use the wanda variant described in the appendix",
    )
    parser.add_argument("--save", type=str, default=None, help="Path to save results.")
    parser.add_argument(
        "--save_model", type=str, default=None, help="Path to save the pruned model."
    )
    parser.add_argument(
        "--save_mask",
        action="store_true",
        default=None,
        help="Path to save the pruned model weight mask.",
    )
    parser.add_argument(
        "--dump_wanda_score", action="store_true", help="Whether to dump wanda scores."
    )

    parser.add_argument("--eval_zero_shot", action="store_true")
    parser.add_argument("--zero_shot_limit", type=int, default=200, help="Limit on the number of examples per task when evaluating zero shot")
    parser.add_argument("--eval_unlearning", action="store_true")
    parser.add_argument("--unlearning_limit", type=int, default=500, help="Limit on the number of examples per task when evaluating unlearning")   
    parser.add_argument("--eval_attack", action="store_true")
    parser.add_argument("--save_attack_res", action="store_true")
    parser.add_argument(
        "--prune_part",
        action="store_true",
        help="whether to only prune the layer with lower jaccard index",
    )
    parser.add_argument(
        "--entangle_prompt_feat",
        dest="disentangle",
        action="store_false",
        help="entangle the prompt and response when computing the wanda score",
    )
    parser.add_argument(
        "--decouple_align_utility",
        action="store_true",
        help="whether to decouple the align and utility when computing the wanda score",
    )
    parser.add_argument(
        "--decouple_align_misalign",
        action="store_true",
        help="whether to decouple the align and misalign when computing the wanda score",
    )
    parser.add_argument(
        "--results_file",
        type=str,
        default="results.json",
        help="Name of the file where the results should be stored. requires suffix .json",
    )
    parser.add_argument(
        "--prune_ratio_threshold",
        type=float,
        default=1,
        help="This is the threshold on the ratio between utility and pruning score below which we start diminishing the weights"
    )
    parser.add_argument(
        "--model_seq_len_thresh",
        type=int,
        default=1024,
        help="Upper bound on model seq len. For methods using backprop this is usually the limit. For other, 8192 should be good."
    )

    # low rank
    parser.add_argument("--rank", type=int, default=10)
    parser.add_argument("--niter", type=int, default=20)

    args = parser.parse_args()

    print("Disentangle:", args.disentangle)

    if args.dump_wanda_score:
        assert args.prune_method in [
            "wanda",
            "wanda_v2",
            "wandg",
        ], "dump_wanda_score only works with wanda wanda_v2 wandg"

    # Setting seeds for reproducibility
    np.random.seed(args.seed)
    torch.random.manual_seed(args.seed)
    
    # Dictionary to store configuration and results
    info_dict = {}
    info_dict.update(vars(args))

    # Handling n:m sparsity
    prune_n, prune_m = 0, 0
    if args.sparsity_type != "unstructured":
        assert (
            args.sparsity_ratio == 0.5
        ), "sparsity ratio must be 0.5 for structured N:M sparsity"
        prune_n, prune_m = map(int, args.sparsity_type.split(":"))

    print(f"loading llm model {args.model}")
    model = get_llm(args.model, args.cache_dir)
    model.eval()
    if model.seqlen > args.model_seq_len_thresh: # without this we run out of memory too easily # only when doing backprop
        model.seqlen = args.model_seq_len_thresh
    tokenizer = AutoTokenizer.from_pretrained(
        modeltype2path[args.model], use_fast=False
    )

    if (args.decouple_align_misalign or args.decouple_align_utility) and (
        tokenizer.pad_token is None
    ):
        tokenizer.add_special_tokens({"pad_token": "[PAD]"})
        model.resize_token_embeddings(len(tokenizer))

    if args.use_diff or args.recover_from_base:
        print(f"WARNING: loading llm BASE model {args.model_base}")
        model_base = get_llm(args.model_base, args.cache_dir)
        model_base.eval()
    else:
        model_base = None

    if args.decouple_align_utility or args.decouple_align_misalign:
        if args.decouple_align_utility:
            print(f"decoupling align and utility, loading extra model{args.model}")
        else:
            print(f"decoupling align and misalign, loading extra model{args.model}")
        model_extra = get_llm(args.model, args.cache_dir)
        model_extra.eval()
        model_extra.resize_token_embeddings(len(tokenizer))
    else:
        model_extra = None

    device = torch.device("cuda:0")
    if (
        "30b" in args.model or "65b" in args.model
    ):  # for 30b and 65b we use device_map to load onto multiple A6000 GPUs, thus the processing here.
        device = model.hf_device_map["lm_head"]
    print("use device ", device)

    if args.save_model:
        model.save_pretrained(args.save_model)
        tokenizer.save_pretrained(args.save_model)

    if args.sparsity_ratio != 0:
        print("pruning starts")
        if args.prune_method == "wanda":
            if args.decouple_align_utility or args.decouple_align_misalign:
                prune_wanda_decouple_activations(
                    args,
                    model,
                    tokenizer,
                    model_base,
                    model_extra,
                    device,
                    prune_n=prune_n,
                    prune_m=prune_m,
                    prune_data=args.prune_data,
                )
            else:
                prune_wanda(
                    args,
                    model,
                    tokenizer,
                    model_base,
                    device,
                    prune_n=prune_n,
                    prune_m=prune_m,
                    prune_data=args.prune_data,
                )
        elif args.prune_method == "magnitude":
            prune_magnitude(
                args,
                model,
                tokenizer,
                model_base,
                device,
                prune_n=prune_n,
                prune_m=prune_m,
            )
        elif args.prune_method == "random":
            prune_random(
                args,
                model,
                tokenizer,
                model_base,
                device,
                prune_n=prune_n,
                prune_m=prune_m,
            )
        elif args.prune_method == "sparsegpt":
            prune_sparsegpt(
                args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m
            )
        elif args.prune_method == "wanda_v2":
            prune_wanda_v2(
                args,
                model,
                tokenizer,
                model_base,
                device,
                prune_n=prune_n,
                prune_m=prune_m,
                prune_data=args.prune_data,
            )
        elif args.prune_method == "wandg":
            prune_wandg(
                args,
                model,
                tokenizer,
                model_base,
                device,
                prune_n=prune_n,
                prune_m=prune_m,
                prune_data=args.prune_data,
            )
        elif args.prune_method == "wandg_set_difference":
            prune_wandg_set_difference(
                args,
                model,
                tokenizer,
                model_base,
                device,
                prune_n=prune_n,
                prune_m=prune_m,
                prune_data=args.prune_data,
                p=args.p,
                q=args.q,
                dynamic_sparsity=False
            )
        elif args.prune_method == "dynamic_sparsity_set_difference":
            prune_wandg_set_difference(
                args,
                model,
                tokenizer,
                model_base,
                device,
                prune_n=prune_n,
                prune_m=prune_m,
                prune_data=args.prune_data,
                p=args.p,
                q=args.q,
                dynamic_sparsity=True
            )
        elif args.prune_method == "score_ratio":
            prune_ratio(
                args,
                model,
                model_base,
                prune_data=args.prune_data,
                )
        elif args.prune_method == "layer_by_layer":
            prune_layer_by_layer(
                args,
                model,
                tokenizer,
            )
        elif args.prune_method == "attention_head":
            prune_attention_head(
                args, model, model_base, device, top_k_heads=args.top_k_heads
            )
        elif "ablate" in args.prune_method:
            prune_ablate(
                args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m
            )
        elif args.prune_method == "low_rank_diff":
            make_low_rank(args,
                model,
                tokenizer,
                prune_data_pos=args.util_data,
                prune_data_neg=args.prune_data,
            )

    if args.prune_method == "low_rank":
        make_low_rank(args, model, tokenizer, device, prune_data=args.prune_data)

    del model_base
    del model_extra

    ################################################################
    print("*" * 30)
    if not args.recover_from_base and args.sparsity_ratio > 0:
        # check_sparsity_layerwise(model)
        sparsity_ratio = check_sparsity(model)
    else:
        sparsity_ratio = args.sparsity_ratio
    print(f"sparsity sanity check {sparsity_ratio:.6f}")
    info_dict["sparsity_ratio"] = sparsity_ratio
    print("*" * 30)
    ################################################################
    ppl_test = eval_ppl(args, model, tokenizer, device)
    # ppl_test = 8.
    info_dict["ppl"] = ppl_test
    print(f"wikitext perplexity {ppl_test}")

    if not os.path.exists(args.save):
        os.makedirs(args.save)
    save_filepath = os.path.join(args.save, f"log_{args.prune_method}.txt")
    if args.save_attack_res:
        if args.prune_method == "attention_head":
            save_attackpath = os.path.join(
                args.save, f"attack_{args.sparsity_ratio:.6f}_top_{args.top_k_heads}"
            )
        else:
            save_attackpath = os.path.join(
                args.save, f"attack_{args.sparsity_ratio:.6f}"
            )
        print(save_attackpath)
        if not os.path.exists(save_attackpath):
            os.makedirs(save_attackpath)
    else:
        save_attackpath = ""
    if not os.path.exists(save_filepath):
        with open(save_filepath, "w") as f:
            if not args.prune_method == "wandg_set_difference":
                print("method\tactual_sparsity\tmetric\tscore\tscore_norm", file=f, flush=True)
                print(
                    f"{args.prune_method}\t{sparsity_ratio:.6f}\tPPL\t{ppl_test:.4f}",
                    file=f,
                    flush=True,
                )
            else:
                print(
                    "method\tactual_sparsity\tp\tq\tmetric\tscore\tscore_norm", file=f, flush=True
                )
                print(
                    f"{args.prune_method}\t{sparsity_ratio:.6f}\t{args.p}\t{args.q}\tPPL\t{ppl_test:.4f}",
                    file=f,
                    flush=True,
                )
    else:
        with open(save_filepath, "a") as f:
            if not args.prune_method == "wandg_set_difference":
                print(
                    f"{args.prune_method}\t{sparsity_ratio:.6f}\tPPL\t{ppl_test:.4f}",
                    file=f,
                    flush=True,
                )
            else:
                print(
                    f"{args.prune_method}\t{sparsity_ratio:.6f}\t{args.p}\t{args.q}\tPPL\t{ppl_test:.4f}",
                    file=f,
                    flush=True,
                )

    if args.save_mask:
        mask = get_mask(model, args.neg_prune)
        mask_folder = os.path.join(args.save, "FT_mask")
        if not os.path.exists(mask_folder):
            os.makedirs(mask_folder)
        if args.neg_prune:
            save_mask_path = os.path.join(
                mask_folder, f"mask_top_{args.sparsity_ratio:.3f}.pt"
            )
        else:
            save_mask_path = os.path.join(
                mask_folder, f"mask_bottom_{args.sparsity_ratio:.3f}.pt"
            )
        torch.save(mask, save_mask_path)
        print(f"Saved weight mask to {save_mask_path}")

    if args.eval_unlearning:
        accelerate = False
        if "30b" in args.model or "65b" in args.model or "70b" in args.model:
            accelerate = True

        num_shot = 0
        results = eval_unlearn(
            modeltype2path[args.model],
            model,
            tokenizer,
            num_shot,
            accelerate,
            limit=args.unlearning_limit, # limit of examples per task use only for testing
        )
        print("********************************")
        print("unlearning evaluation results")
        sum_acc = 0
        with open(save_filepath, "a") as f:
            for k, v in results["results"].items():
                print(
                    f"{args.prune_method}\t{sparsity_ratio:.6f}\t{args.p}\t{args.q}\t{k}\t{v['acc']:.4f}\t{v['acc_norm']:.4f}",
                    file=f,
                    flush=True,
                )
                info_dict[k] = v["acc"]

        print(results)

    # if args.eval_attack:
    #     # note: since vLLM only supports loading from the path, we need to save the pruned model first for faster evaluation. We can reuse this temp folder to save disk spaces
    #     pruned_path = os.path.join(
    #         args.save,
    #         f"{args.prune_method}_usediff_{args.use_diff}_recover_{args.recover_from_base}",
    #     )
        # model.save_pretrained(pruned_path)
        # vllm_model = LLM(
        #     model=pruned_path,
        #     tokenizer=modeltype2path[args.model],
        #     dtype="bfloat16",
        #     swap_space=128,
        # )
        # if args.decouple_align_utility or args.decouple_align_misalign:
        #     vllm_model.llm_engine.tokenizer.add_special_tokens({"pad_token": "[PAD]"})
        # for include_inst in [True, False]:
        #     suffix = "inst_" if include_inst else "no_inst_"
        #     print("********************************")
        #     # ASR_basic
        #     score = eval_attack(
        #         vllm_model,
        #         tokenizer,
        #         num_sampled=1,
        #         add_sys_prompt=True,
        #         do_sample=False,
        #         save_attack_res=args.save_attack_res,
        #         include_inst=include_inst,
        #         filename=os.path.join(save_attackpath, f"{suffix}basic.jsonl"),
        #     )
        #     print(f"attack evaluation results ({suffix}basic): {score:.4f}")
        #     with open(save_filepath, "a") as f:
        #         if not args.prune_method == "wandg_set_difference":
        #             print(
        #                 f"{args.prune_method}\t{sparsity_ratio:.6f}\t{suffix}ASR_basic\t{score:.4f}",
        #                 file=f,
        #                 flush=True,
        #             )
        #         else:
        #             print(
        #                 f"{args.prune_method}\t{sparsity_ratio:.6f}\t{args.p}\t{args.q}\t{suffix}ASR_basic\t{score:.4f}",
        #                 file=f,
        #                 flush=True,
        #             )
        #     # ASR_basic_no_sys
        #     print("********************************")
        #     score = eval_attack(
        #         vllm_model,
        #         tokenizer,
        #         num_sampled=1,
        #         add_sys_prompt=False,
        #         do_sample=False,
        #         save_attack_res=args.save_attack_res,
        #         include_inst=include_inst,
        #         filename=os.path.join(save_attackpath, f"{suffix}basic_no_sys.jsonl"),
        #     )
        #     print(
        #         f"attack evaluation results ({suffix}basic, no sys prompt): {score:.4f}"
        #     )
        #     with open(save_filepath, "a") as f:
        #         if not args.prune_method == "wandg_set_difference":
        #             print(
        #                 f"{args.prune_method}\t{sparsity_ratio:.6f}\t{suffix}ASR_basic_nosys\t{score:.4f}",
        #                 file=f,
        #                 flush=True,
        #             )
        #         else:
        #             print(
        #                 f"{args.prune_method}\t{sparsity_ratio:.6f}\t{args.p}\t{args.q}\t{suffix}ASR_basic_nosys\t{score:.4f}",
        #                 file=f,
        #                 flush=True,
        #             )
        #     # seems that llama2-13b may run into error on this :(
        #     # ASR_multiple_nosys
        #     if args.model == "llama2-7b-chat-hf" or "llama2-13b-chat-hf":
        #         print("********************************")
        #         score = eval_attack(
        #             vllm_model,
        #             tokenizer,
        #             num_sampled=5,
        #             add_sys_prompt=False,
        #             do_sample=True,
        #             save_attack_res=args.save_attack_res,
        #             include_inst=include_inst,
        #             filename=os.path.join(
        #                 save_attackpath, f"{suffix}multiple_no_sys.jsonl"
        #             ),
        #         )
        #         print(
        #             f"attack evaluation results ({suffix}multiple, no sys prompt): {score:.4f}"
        #         )
        #         with open(save_filepath, "a") as f:
        #             if not args.prune_method == "wandg_set_difference":
        #                 print(
        #                     f"{args.prune_method}\t{sparsity_ratio:.6f}\t{suffix}ASR_multiple_nosys\t{score:.4f}",
        #                     file=f,
        #                     flush=True,
        #                 )
        #             else:
        #                 print(
        #                     f"{args.prune_method}\t{sparsity_ratio:.6f}\t{args.p}\t{args.q}\t{suffix}ASR_multiple_nosys\t{score:.4f}",
        #                     file=f,
        #                     flush=True,
        #                 )
        # # ASR_gcg
        # score = eval_attack(
        #     vllm_model,
        #     tokenizer,
        #     num_sampled=1,
        #     add_sys_prompt=False,
        #     gcg=True,
        #     do_sample=False,
        #     save_attack_res=args.save_attack_res,
        #     include_inst=True,
        #     filename=os.path.join(save_attackpath, f"gcg.jsonl"),
        # )
        # print(f"attack evaluation results (gcg): {score:.4f}")
        # with open(save_filepath, "a") as f:
        #     if not args.prune_method == "wandg_set_difference":
        #         print(
        #             f"{args.prune_method}\t{sparsity_ratio:.6f}\tASR_gcg\t{score:.4f}",
        #             file=f,
        #             flush=True,
        #         )
        #     else:
        #         print(
        #             f"{args.prune_method}\t{sparsity_ratio:.6f}\t{args.p}\t{args.q}\tASR_gcg\t{score:.4f}",
        #             file=f,
        #             flush=True,
        #         )
        # del vllm_model

    if args.eval_zero_shot:
        accelerate = False
        if "30b" in args.model or "65b" in args.model or "70b" in args.model:
            accelerate = True

        task_list = [
            "boolq",
            "rte",
            "hellaswag",
            "winogrande",
            "arc_challenge",
            "openbookqa",
        ]
        num_shot = 0
        results = eval_zero_shot(
            modeltype2path[args.model],
            model,
            tokenizer,
            task_list,
            num_shot,
            accelerate,
            limit=args.zero_shot_limit,
        )
        print("********************************")
        print("zero_shot evaluation results")
        sum_acc = 0
        with open(save_filepath, "a") as f:
            for k, v in results["results"].items():
                if not args.prune_method == "wandg_set_difference":
                    print(
                        f"{args.prune_method}\t{sparsity_ratio:.6f}\t{k}\t{v['acc']:.4f}",
                        file=f,
                        flush=True,
                    )
                else:
                    print(
                        f"{args.prune_method}\t{sparsity_ratio:.6f}\t{args.p}\t{args.q}\t{k}\t{v['acc']:.4f}",
                        file=f,
                        flush=True,
                    )
                sum_acc += v["acc"]
                info_dict[k] = v["acc"]
            if not args.prune_method == "wandg_set_difference":
                print(
                    f"{args.prune_method}\t{sparsity_ratio:.6f}\taveraged\t{sum_acc/len(task_list):.4f}",
                    file=f,
                    flush=True,
                )
            else:
                print(
                    f"{args.prune_method}\t{sparsity_ratio:.6f}\t{args.p}\t{args.q}\taveraged\t{sum_acc/len(task_list):.4f}",
                    file=f,
                    flush=True,
                )
        info_dict["averaged"] = sum_acc/len(task_list)
        print(results)

    # save the results
    result_path = os.path.join(RESULTS_DIR, args.results_file)
    mode = "a" if os.path.isfile(result_path) else "w"
    with open(result_path, mode) as f:
        json.dump(info_dict, f)
        f.write('\n')

if __name__ == "__main__":
    main()
