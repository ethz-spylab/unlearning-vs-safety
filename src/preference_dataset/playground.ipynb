{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "import transformers\n",
    "import openai\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\", split=\"all\") # we do not care about train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrefusal_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m REFUSAL_LIST\n\u001b[1;32m      2\u001b[0m SEED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_prompt\u001b[39m(sample: \u001b[38;5;28mstr\u001b[39m, contex: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from .refusal_list import REFUSAL_LIST\n",
    "SEED = 42\n",
    "context = \"biology\"\n",
    "\n",
    "def format_prompt(sample: str, contex: str) -> str:\n",
    "    prompt = f\"The following are multiple choice questions (with answers) about {contex}.\\n\\n\"\n",
    "    prompt += f\"{sample['question']}\\n\"\n",
    "    ABCDEF = list(\"ABCDEF\")\n",
    "    for i, option in enumerate(sample[\"choices\"][\"text\"]):\n",
    "        prompt += f\"{ABCDEF[i]}. {option}\\n\"\n",
    "    prompt += \"Answer:\"\n",
    "    \n",
    "    answer_id = sample[\"choices\"][\"label\"].index(sample[\"answerKey\"])\n",
    "    answer = f\"{ABCDEF[answer_id]}. {sample[\"choices\"][\"text\"][answer_id]}\\n\"\n",
    "    return prompt, answer\n",
    "\n",
    "mc_retain_dpo = {\n",
    "        \"prompt\": [],\n",
    "        \"chosen\": [],\n",
    "        \"rejected\": [],\n",
    "    }\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "for sample in ds:\n",
    "    prompt, answer = format_prompt(sample, context)\n",
    "    \n",
    "    mc_retain_dpo[\"prompt\"].append(prompt)\n",
    "    mc_retain_dpo[\"chosen\"].append(answer)\n",
    "    mc_retain_dpo[\"rejected\"].append(random.choice(REFUSAL_LIST))\n",
    "\n",
    "with open(os.path.join(CUSTOM_DATASET_DIR, \"mc_retain_dpo.json\"), \"w\") as f:\n",
    "            json.dump(bio_forget_dpo, f)\n",
    "    \n",
    "# upload to huggingface\n",
    "final_bio = Dataset.from_dict(bio_forget_dpo)\n",
    "final_bio.push_to_hub(f\"{HF_USERNAME}/bio_forget_dpo\", private=True) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL = \"gpt-4o-mini-2024-07-18\"#\"gpt-4o-2024-08-06\"\n",
    "SYSTEM_PROMPT = r\"\"\"\n",
    "You are a large language model tasked with creating multiple-choice questions based on an article. Each question is supposed to have four answers: A, B, C, D. Only one of these options is supposed to be correct, other three are supposed to be plausible but incorrect distractors. Your goal is to generate clear, concise questions that test knowledge contained within the article. You are not supposed to test text comprehension but rather concepts within it. Make sure that questions can be answered without an access to the article. Do not ask about data and statistics such as how many people died from polio in total. Instead ask 'Which of the following viruses causes polio?'. Never refer to the article directly. Always provide a correct answer and corresponding explanation. Make sure that the answer is one of the options. Don't include option letters A,B,C,D as part of each option. Instead it should be a phrase. Provide a list of 10 such questions. \n",
    "\n",
    "### EXAMPLE\n",
    "\n",
    "ARTICLE:\n",
    "Introduction {#bjh13767-sec-0001} ============ Purine analogues have revolutionized the treatment of hairy cell leukaemia (HCL) with overall responses in more than 85% of patients and a median progression‐free survival (PFS) of up to 15 years (Else *et al*, [2009](#bjh13767-bib-0004){ref-type=\"ref\"}; Grever, [2010](#bjh13767-bib-0006){ref-type=\"ref\"}). Despite the success of pentostatin and cladribine, PFS curves have revealed no plateau in long‐term follow‐up studies (Else *et al*, [2009](#bjh13767-bib-0004){ref-type=\"ref\"}). In addition, there is a small subset of patients with primary refractory disease. Second and third line treatment with single‐agent purine analogues produce lower complete remission (CR) rates and shorter PFS (Chadha *et al*, [2005](#bjh13767-bib-0003){ref-type=\"ref\"}). In that context, the combination of pentostatin or cladribine with rituximab was tested and found to be effective (Else *et al*, [2011](#bjh13767-bib-0005){ref-type=\"ref\"}), and is now recommended in the UK for recurrent or refractory HCL (Jones *et al*, [2012](#bjh13767-bib-0007){ref-type=\"ref\"}). \n",
    "\n",
    "QUESTION 1:\n",
    "Which treatment approach has shown to be highly effective in achieving long-term progression-free survival (PFS) in patients with hairy cell leukemia (HCL)?\n",
    "\n",
    "OPTION A:\n",
    "Stem cell transplantation\n",
    "OPTION B:\n",
    "Combination of chemotherapy and radiation\n",
    "OPTION C:\n",
    "Purine analogues\n",
    "OPTION D:\n",
    "Monoclonal antibody therapy alone\n",
    "\n",
    "ANSWER:\n",
    "Purine analogues\n",
    "\n",
    "EXPLANATION:\n",
    "Purine analogues, specifically cladribine (2-CdA) and pentostatin, have been the standard of care for HCL and have been associated with high rates of complete remission and long-term PFS.\n",
    "\n",
    "### END OF EXAMPLE\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MCQ(BaseModel):\n",
    "    question: str\n",
    "    options: list[str]\n",
    "    answer: str\n",
    "    explanation: str\n",
    "    \n",
    "class ListMCQ(BaseModel):\n",
    "    multiple_choice_questions: list[MCQ]\n",
    "\n",
    "def format_question(mc_question: MCQ, context: str) -> str:\n",
    "    assert mc_question.answer in mc_question.options, \"Answer not in options\"\n",
    "    assert len(mc_question.options) == 4, \"There should be exactly 4 options\"\n",
    "    \n",
    "    question = f\"The following are multiple choice questions (with answers) about {context}.\\n\\n\"\n",
    "    question += f\"{mc_question.question}\\n\"\n",
    "    ABCD = list(\"ABCD\")\n",
    "    random.shuffle(mc_question.options)\n",
    "    for i, option in enumerate(mc_question.options):\n",
    "        question += f\"{ABCD[i]}. {option}\\n\"\n",
    "    question += \"Answer:\"\n",
    "    \n",
    "    correct_index = mc_question.options.index(mc_question.answer)\n",
    "    answer = f\"{ABCD[correct_index]}. {mc_question.answer}\\n\"\n",
    "    explanation = f\"Explanation:\\n{mc_question.explanation}\"\n",
    "    return question, answer, explanation\n",
    "\n",
    "def prompt_gpt(client, article: str, context: str) -> Optional[MCQ]:\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f\"Write 10 multiple-choice questions in the field of {context} based on the following article. Try to focus on {context} only and avoid related fields.\\nARTICLE:\\n{article}\"},\n",
    "        ],\n",
    "        response_format=ListMCQ,\n",
    "    )\n",
    "    mc_question = completion.choices[0].message\n",
    "    \n",
    "    return None if (mc_question.refusal) else mc_question.parsed, completion.usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ..util.globals import BIO_FORGET_CORPUS_PATH\n",
    "from datasets import load_dataset\n",
    "BIO_FORGET_CORPUS_PATH = \"/home/jlucki/project_data/wmdp_localization/data/bio_forget/\"\n",
    "\n",
    "bio_forget = load_dataset(\"json\", data_dir=str(BIO_FORGET_CORPUS_PATH), split=\"train\")\n",
    "context = \"biology\"\n",
    "client = openai.OpenAI(organization=\"org-HprxAmwgEaFAinLZgxlt5lqy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, usage = prompt_gpt(client, bio_forget[0][\"text\"], context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=831, prompt_tokens=4233, total_tokens=5064)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16027"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bio_forget[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are multiple choice questions (with answers) about biology.\n",
      "\n",
      "What type of virus is SARS-CoV-2?\n",
      "A. Bacteriophage\n",
      "B. Fungi\n",
      "C. Coronavirus\n",
      "D. Retrovirus\n",
      "Answer:\n",
      "C. Coronavirus\n",
      "\n",
      "Explanation:\n",
      "SARS-CoV-2 is classified as a coronavirus, which is a group of viruses known for causing respiratory infections in humans.\n",
      "The following are multiple choice questions (with answers) about biology.\n",
      "\n",
      "What does the reproduction number R0 indicate in an epidemic?\n",
      "A. The rate of vaccination coverage\n",
      "B. The number of fatalities per case\n",
      "C. The total number of cases\n",
      "D. The average number of secondary cases generated per case\n",
      "Answer:\n",
      "D. The average number of secondary cases generated per case\n",
      "\n",
      "Explanation:\n",
      "R0 measures the potential for an outbreak to spread in a fully susceptible population, indicating how many secondary cases one infected individual will create.\n",
      "The following are multiple choice questions (with answers) about biology.\n",
      "\n",
      "In which of the following populations is the case fatality rate higher for COVID-19?\n",
      "A. Males aged 50-59\n",
      "B. Older adults over 80 years\n",
      "C. Younger adults aged 18-29\n",
      "D. Children under 10\n",
      "Answer:\n",
      "B. Older adults over 80 years\n",
      "\n",
      "Explanation:\n",
      "The article states that the case fatality rate increases significantly with age, particularly among those aged 80 and older.\n",
      "The following are multiple choice questions (with answers) about biology.\n",
      "\n",
      "What is a notable characteristic of the outbreak linked to the Shincheonji Church of Jesus?\n",
      "A. It was contained without any intervention\n",
      "B. It was the largest cluster in South Korea\n",
      "C. It affected primarily children\n",
      "D. All cases were mild\n",
      "Answer:\n",
      "B. It was the largest cluster in South Korea\n",
      "\n",
      "Explanation:\n",
      "The Shincheonji Church of Jesus cluster was reported to have resulted in the highest number of confirmed cases in South Korea.\n",
      "The following are multiple choice questions (with answers) about biology.\n",
      "\n",
      "What are superspreading events in the context of infectious disease outbreaks?\n",
      "A. Events that lead to total eradication of the virus\n",
      "B. Events where one infected individual transmits the virus to a disproportionately large number of people\n",
      "C. Events where vaccines are distributed\n",
      "D. Events where virus strains mutate\n",
      "Answer:\n",
      "B. Events where one infected individual transmits the virus to a disproportionately large number of people\n",
      "\n",
      "Explanation:\n",
      "Superspreading events refer to situations where a single infected person infects many others, significantly increasing the number of cases.\n",
      "The following are multiple choice questions (with answers) about biology.\n",
      "\n",
      "Which measure can help decrease the effective reproduction number (Rt) during an outbreak?\n",
      "A. Implementing social distancing\n",
      "B. Increasing population density\n",
      "C. Encouraging public gatherings\n",
      "D. Expanding contact tracing\n",
      "Answer:\n",
      "A. Implementing social distancing\n",
      "\n",
      "Explanation:\n",
      "Implementing social distancing is a public health measure intended to reduce the transmission of the virus and subsequently decrease the reproductive number.\n",
      "The following are multiple choice questions (with answers) about biology.\n",
      "\n",
      "Which of the following factors can affect the effective reproduction number (Rt) of a virus?\n",
      "A. Weather conditions solely\n",
      "B. Time of day\n",
      "C. Behavior changes and public health measures\n",
      "D. Virus mutations only\n",
      "Answer:\n",
      "C. Behavior changes and public health measures\n",
      "\n",
      "Explanation:\n",
      "The effective reproduction number is influenced by changes in behavior of the population, as well as public health interventions aimed at reducing transmission.\n",
      "The following are multiple choice questions (with answers) about biology.\n",
      "\n",
      "What type of distribution is assumed for the generation interval in estimating the reproduction number (Rt)?\n",
      "A. Normal distribution\n",
      "B. Poisson distribution\n",
      "C. Uniform distribution\n",
      "D. Gamma distribution\n",
      "Answer:\n",
      "D. Gamma distribution\n",
      "\n",
      "Explanation:\n",
      "The article mentions that the generation interval is assumed to follow a gamma distribution for modeling transmission dynamics.\n",
      "The following are multiple choice questions (with answers) about biology.\n",
      "\n",
      "Which population has been reported to exhibit severe symptoms from COVID-19?\n",
      "A. Only elderly adults\n",
      "B. Healthy young adults\n",
      "C. Individuals with underlying health conditions\n",
      "D. Pregnant women\n",
      "Answer:\n",
      "C. Individuals with underlying health conditions\n",
      "\n",
      "Explanation:\n",
      "The article highlights that COVID-19 can lead to severe disease particularly in individuals with underlying health issues such as cardiovascular disease and diabetes.\n",
      "The following are multiple choice questions (with answers) about biology.\n",
      "\n",
      "What strategy did the Korean government use to control the COVID-19 outbreak?\n",
      "A. Reducing healthcare funding\n",
      "B. Raising COVID-19 alert levels and social distancing\n",
      "C. Allowing unrestricted travel\n",
      "D. Promoting public gatherings\n",
      "Answer:\n",
      "B. Raising COVID-19 alert levels and social distancing\n",
      "\n",
      "Explanation:\n",
      "The Korean government raised the alert level and implemented social distancing measures to control the spread of COVID-19.\n"
     ]
    }
   ],
   "source": [
    "for i in answer.multiple_choice_questions:\n",
    "    q, a, e = format_question(i, context)\n",
    "    print(q)\n",
    "    print(a)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_list_mcq(mcq: ListMCQ) -> str:\n",
    "    question, answer, explanation = format_question(mcq, context)\n",
    "    return f\"{question}\\n{answer}\\n{explanation}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question='Which viral protein in SARS-CoV-2 is identified as a key target for antiviral drug discovery due to its role in host mRNA degradation and suppression of interferon expression?' options=['Non-structural protein 1 (nsp1)', 'Spike protein', 'Envelope protein', 'Nucleocapsid protein'] answer='Non-structural protein 1 (nsp1)' explanation='Non-structural protein 1 (nsp1) in SARS-CoV-2 is a crucial virulence factor responsible for host mRNA degradation and suppression of interferon expression, making it a key target for antiviral drug discovery.'\n"
     ]
    }
   ],
   "source": [
    "class MCQ(BaseModel):\n",
    "    question: str\n",
    "    options: list[str]\n",
    "    answer: str\n",
    "    explanation: str\n",
    "\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=openai_model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Write a multiple-choice question in the field of {context} based on the following article.\\nARTICLE:\\n{bio_forget[1]['abstract']}\"},\n",
    "    ],\n",
    "    response_format=MCQ,\n",
    ")\n",
    "\n",
    "mc_question = completion.choices[0].message\n",
    "\n",
    "# If the model refuses to respond, you will get a refusal message\n",
    "if (mc_question.refusal):\n",
    "    print(mc_question.refusal)\n",
    "else:\n",
    "    print(mc_question.parsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
